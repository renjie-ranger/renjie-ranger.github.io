
# üìù Publications 

## üß† Large Language Model & Reasoning

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2024</div><img src='images/olympiadbench.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems](https://arxiv.org/abs/2402.14008) \\
**Renjie Luo**, Chaoqun He, Yuzhuo Bai, Shengding Hu, Xu Han, et al.

[**Project**](https://github.com/OpenBMB/OlympiadBench) <strong><span class='show_paper_citations' data='X7Fy--gAAAAJ:u5HHmVD_uO8C'></span></strong>

- **Academic Impact**: A challenging benchmark containing Olympiad-level contest questions in math and physics, aiming for the evaluation of GPT5-level LLMs.
- **High Citations**: This work has received **278 citations**, demonstrating its significant impact in the AGI evaluation community.
- **Bilingual & Multimodal**: Features both English and Chinese problems with comprehensive multimodal scientific content.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2024</div><img src='images/ultraeval.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[UltraEval: A Lightweight Platform for Flexible and Comprehensive Evaluation for LLMs](https://arxiv.org/abs/2404.07584) \\
**Renjie Luo**, Chaoqun He, Yuzhuo Bai, Shengding Hu, Xu Han, et al.

[**Project**](https://github.com/OpenBMB/UltraEval) <strong><span class='show_paper_citations' data='X7Fy--gAAAAJ:u-x6o8ySG0sC'></span></strong>

- **Open Source Impact**: An automated evaluation framework for large language models, multi-dimensional, with user-friendly and highly customizable evaluation strategies.
- **Community Adoption**: **200+ stars** on GitHub and **17 citations**, widely used by researchers for LLM evaluation.
- **Comprehensive Features**: Supports flexible evaluation strategies with highly customizable pipeline design.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Under Review</div><img src='images/cot_training.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Through the Valley: Path to Effective Long CoT Training for Small Language Models](https://arxiv.org/abs/2401.XXXXX) \\
**Renjie Luo**, Tianyu Pang, Chao Du, Wei Lu

[**Project**](https://github.com/renjie-ranger/LongCoT) 

- **Novel Approach**: Studies how long Chain-of-Thought training data affects small language models, discovering "Long CoT Degradation" phenomenon.
- **Important Findings**: Demonstrates that limited training hurts performance and analyzes the cause as error accumulation from overly verbose outputs.
- **Practical Solutions**: Shows that sufficient long CoT data improves subsequent RL performance significantly.
</div>
</div>

## üî¨ Research Projects

- **RL-Enhanced LLM Fine-Tuning for Mathematical Reasoning**: Independently led the project, integrating Q-learning into the fine-tuning process of large language models (LLMs) to optimize Q-value estimation during training. Conducted experiments, fine-tuned hyperparameters, and analyzed results to validate improvements in reasoning capabilities.

- **LLM Automated Evaluation Framework**: Developed an open-source evaluation framework (UltraEval) for large language models, featuring multi-dimensional assessment with user-friendly and highly customizable evaluation strategies. **200+ stars** on GitHub.

- **Multi-GPU Evaluation Acceleration Framework**: Python code implementation for evaluation acceleration built with Gunicorn and Flask, suitable for local inference of language models, integrated in the UltraEval framework.

- **Multimodal Open-domain Dialog Systems Evaluation Benchmark Design**: Research-oriented, open source, the first benchmark framework in the field used for evaluating the performance of multimodal open-domain dialogue systems.