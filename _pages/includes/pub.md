
# üìù Publications 

## üß† Large Language Model & Reasoning

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2024</div><img src='pub_images/OlympiadBench.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems](https://arxiv.org/abs/2402.14008) \\
Chaoqun He, **Renjie Luo**, Yuzhuo Bai, Shengding Hu, Xu Han, et al.

[**Project**](https://github.com/OpenBMB/OlympiadBench) <strong><span class='show_paper_citations' data='X7Fy--gAAAAJ:u5HHmVD_uO8C'></span></strong>

- **Academic Impact**: A challenging benchmark containing Olympiad-level contest questions in math and physics, aiming for the evaluation of GPT5-level LLMs.
- **Bilingual & Multimodal**: Features both English and Chinese problems with comprehensive multimodal scientific content.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2024</div><img src='pub_images/UltraEval.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[UltraEval: A Lightweight Platform for Flexible and Comprehensive Evaluation for LLMs](https://arxiv.org/abs/2404.07584) \\
Chaoqun He, **Renjie Luo**, Yuzhuo Bai, Shengding Hu, Xu Han, et al.

[**Project**](https://github.com/OpenBMB/UltraEval) <strong><span class='show_paper_citations' data='X7Fy--gAAAAJ:u-x6o8ySG0sC'></span></strong>

- **Open Source Impact**: An automated evaluation framework for large language models, multi-dimensional, with user-friendly and highly customizable evaluation strategies.
- **Community Adoption**: **200+ stars** on GitHub and **17 citations**, widely used by researchers for LLM evaluation.
- **Comprehensive Features**: Supports flexible evaluation strategies with highly customizable pipeline design.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Under Review</div><img src='pub_images/through_the_valley.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Through the Valley: Path to Effective Long CoT Training for Small Language Models](https://arxiv.org/abs/2506.07712) \\
**Renjie Luo**, Jiaxi Li, Chen Huang, Wei Lu

- **Novel Approach**: Studies how long Chain-of-Thought training data affects small language models, discovering "Long CoT Degradation" phenomenon.
- **Important Findings**: Demonstrates that limited training hurts performance and analyzes the cause as error accumulation from overly verbose outputs.
- **Practical Solutions**: Shows that sufficient long CoT data improves subsequent RL performance significantly.
</div>
</div>