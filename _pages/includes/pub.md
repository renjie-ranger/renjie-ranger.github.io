
# üìù Publications 

## üß† LLM Reasoning

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2025</div><img src='pub_images/verbal_feedback.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Language Models Can Learn from Verbal Feedback without Scalar Rewards](https://arxiv.org/pdf/2509.22638) \\
**Renjie Luo**\*, Zichen Liu, Xiangyan Liu, Chao Du, Min Lin,  
Wenhu Chen, Wei Lu, Tianyu Pang*

[**Project**](https://github.com/sail-sg/feedback-conditional-policy) <strong><span class='show_paper_citations' data='X7Fy--gAAAAJ:u5HHmVD_uO8C'></span></strong>

- **TLDR**: üöÄ We show that LLMs can directly learn from **verbal feedback** ‚Äî no scalar rewards required.
- **Method**: We propose the **Feedback-Conditional Policy (FCP)** ‚Äî treating feedback as a **conditioning signal**.  
  - **Offline stage**: Learn from response‚Äìfeedback pairs via simple MLE.  
  - **Online stage**: Bootstrap with fresh critiques, refining the policy iteratively.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EMNLP 2025</div><img src='pub_images/through_the_valley.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Through the Valley: Path to Effective Long CoT Training for Small Language Models](https://arxiv.org/abs/2506.07712) \\
**Renjie Luo**, Jiaxi Li, Chen Huang, Wei Lu

- **TLDR**: We reveal the "Long CoT Degradation" phenomenon where small language models (‚â§3B) suffer performance drops when trained on limited long chain-of-thought data, and propose effective training strategie (via RLVR) to overcome this challenge.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2024</div><img src='pub_images/OlympiadBench.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems](https://arxiv.org/abs/2402.14008) \\
Chaoqun He, **Renjie Luo**, Yuzhuo Bai, Shengding Hu, Xu Han, et al.

[**Project**](https://github.com/OpenBMB/OlympiadBench) <strong><span class='show_paper_citations' data='X7Fy--gAAAAJ:u5HHmVD_uO8C'></span></strong>

- This work is widely used by RL&Reasoning community, such as [SimpleRL-Zoo](https://arxiv.org/abs/2503.18892), [Dr.GRPO](https://arxiv.org/pdf/2503.20783), [Seed 1.5-VL](https://arxiv.org/pdf/2505.07062), etc.

</div>
</div>

## ‚öôÔ∏è LLM Evaluation Framework

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2024</div><img src='pub_images/UltraEval.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[UltraEval: A Lightweight Platform for Flexible and Comprehensive Evaluation for LLMs](https://arxiv.org/abs/2404.07584) \\
Chaoqun He, **Renjie Luo**, Shengding Hu, Yuanqian Zhao, Jie Zhou, et al.

[**Project**](https://github.com/OpenBMB/UltraEval) <strong><span class='show_paper_citations' data='X7Fy--gAAAAJ:u-x6o8ySG0sC'></span></strong>

- **Open Source Impact**: An automated evaluation framework for large language models, multi-dimensional, with user-friendly and highly customizable evaluation strategies.
- **Community Adoption**: **200+ stars** on GitHub, widely used by researchers for LLM evaluation.
- **Comprehensive Features**: Supports flexible evaluation strategies with highly customizable pipeline design.
</div>
</div>
