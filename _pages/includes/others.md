
# ðŸ“– Educations
- *2025.01 - Present*, Ph.D. Student, Artificial Intelligence, Singapore University of Technology and Design (SUTD), Singapore. Supervised by Prof. Wei Lu.
- *2019.09 - 2024.06*, Bachelor, Artificial Intelligence Engineering, Beihang University, Beijing, China. GPA: 3.64/4.0.

# ðŸ’» Internships
- *2024.08 - 2024.12*, **REDnote Inc.**, Internship Trainee in LLM Reasoning
  - **RL-Enhanced LLM Fine-Tuning for Mathematical Reasoning**: Independently led the project, integrating Q-learning into the fine-tuning process of large language models (LLMs) to optimize Q-value estimation during training. Conducted experiments, fine-tuned hyperparameters, and analyzed results to validate improvements in reasoning capabilities.
  
- *2023.12 - 2024.08*, **Natural Language Processing Lab at Tsinghua University (THUNLP)**, Internship Trainee in NLP Research for Large Language Model
  - **Olympiad-level Benchmark for LLM**: Led topic research, data investigation, collection, cleaning, and annotation strategy for the OlympiadBench project. Managed task distribution, executed nearly half of all annotations, designed automatic scoring pipeline tailored to data specifics, and authored the dataset section in the publication.
  
- *2023.06 - 2023.12*, **ModelBest Inc.**, Internship Trainee in NLP Algorithms
  - **LLM Performance Evaluation**: Led the assessment of large language models, including the establishment of an automated evaluation framework. Gained great improvement on coding proficiency, teamwork collaboration, communication ability.
  
- *2021.11 - 2022.02*, **Beijing Institute for General Artificial Intelligence**, Intern for NLP Research Group
  - **Research Exposure**: Gained a foundational understanding of the scientific research workflow during my initial internship experience. Tasked with processing extensive datasets and configuring the model environment.

# ðŸ’¬ Languages
- **Chinese**: Native
- **English**: Fluent (IELTS 7.5/9)
- **Japanese**: Intermediate
- **Cantonese**: Advanced

# ðŸ”¬ Research Interests
- **Large Language Model Post-training**: Focusing on improving the training strategies and methodologies for large language models after pre-training.
- **Mathematical Reasoning**: Developing and enhancing reasoning capabilities of AI models, particularly in mathematical problem-solving.
- **LLM Evaluation**: Creating comprehensive and reliable evaluation frameworks for assessing the performance of large language models.
- **Chain-of-Thought Learning**: Investigating how different CoT training strategies affect model performance and reasoning abilities.
